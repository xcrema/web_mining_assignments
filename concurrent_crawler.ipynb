{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "class MultiThreadScraper:\n",
    "\n",
    "    def __init__(self, base_url):\n",
    "\n",
    "        self.base_url = base_url\n",
    "        self.root_url = '{}://{}'.format(urlparse(self.base_url).scheme, urlparse(self.base_url).netloc)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=20)\n",
    "        self.scraped_pages = set([])\n",
    "        self.to_crawl = Queue()\n",
    "        self.to_crawl.put(self.base_url)\n",
    "\n",
    "    def parse_links(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if url.startswith('/') or url.startswith(self.root_url):\n",
    "                url = urljoin(self.root_url, url)\n",
    "                if url not in self.scraped_pages:\n",
    "                    self.to_crawl.put(url)\n",
    "\n",
    "    def scrape_info(self, html):\n",
    "        return\n",
    "\n",
    "    def post_scrape_callback(self, res):\n",
    "        result = res.result()\n",
    "        if result and result.status_code == 200:\n",
    "            self.parse_links(result.text)\n",
    "            self.scrape_info(result.text)\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=(3, 30))\n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            return\n",
    "\n",
    "    def run_scraper(self):\n",
    "        while True:\n",
    "            try:\n",
    "                target_url = self.to_crawl.get(timeout=60)\n",
    "                if target_url not in self.scraped_pages:\n",
    "                    print(\"Scraping URL: {}\".format(target_url))\n",
    "                    self.scraped_pages.add(target_url)\n",
    "                    job = self.pool.submit(self.scrape_page, target_url)\n",
    "                    job.add_done_callback(self.post_scrape_callback)\n",
    "            except Empty:\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    s = MultiThreadScraper(\"https://www.w3schools.com/\") \n",
    "    s.run_scraper()\n",
    "    end_time = time.time()\n",
    "    print(end_time-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ConcurrentListCrawler(object):\n",
    "\n",
    "    def __init__(self, domain, url_list, threads):\n",
    "        \n",
    "        self.domain = domain\n",
    "        self.urls = url_list\n",
    "        self.company = list()\n",
    "        self.max_threads = threads\n",
    "\n",
    "    def __make_request(self, url):\n",
    "        try:\n",
    "            r = requests.get(url=url, timeout=20)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.Timeout:\n",
    "            r = requests.get(url=url, timeout=60)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            r = requests.get(url=url, timeout=60)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise e\n",
    "        return r.url, r.text\n",
    "\n",
    "    def __parse_results(self, shala, html):\n",
    "\n",
    "        try:\n",
    "            while(shala!=None):\n",
    "                html_soup = BeautifulSoup(html, 'html.parser')\n",
    "                company_containers = html_soup.find_all('a', class_ = \"link_display_like_text\")\n",
    "                first = company_containers[0]\n",
    "                for comp in company_containers:\n",
    "                    self.company.append(comp.contents[0].strip())\n",
    "                next_page_sec = html_soup.find_all('ul', class_ = \"pager\")\n",
    "                next_page = (next_page_sec[0].find_all('a')[1]['href'])\n",
    "                if next_page == \"#\":\n",
    "                    shala = None\n",
    "                else:\n",
    "                    shala = self.domain+next_page\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def wrapper(self, url):\n",
    "        url, html = self.__make_request(url)\n",
    "        self.__parse_results(url, html)\n",
    "\n",
    "    def run_script(self):\n",
    "        with ThreadPoolExecutor(max_workers=min(len(self.urls),self.max_threads)) as Executor:\n",
    "            jobs = [Executor.submit(self.wrapper, u) for u in self.urls]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example = ConcurrentListCrawler(\"https://internshala.com\", ['https://internshala.com/internships/computer%20science-internship-in-delhi,greater%20noida,gurgaon,gurugram,new%20delhi,noida,south%20west%20delhi,north%20delhi',\n",
    "                                     'https://internshala.com/internships/marketing-internship-in-delhi,greater%20noida,gurgaon,noida,south%20west%20delhi,gurugram,new%20delhi,north%20delhi',\n",
    "                                    ], 5)\n",
    "    example.run_script()\n",
    "    print(example.company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
