{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: ACHYUT TRIPATHI\n",
    "# REGNO: 17BCE0954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2: Write a program to show the implementation of Elias Delta and Golomb Encoding-decoding (both for m = 2N and M ≠ 2N) techniques. (x=37, b=11, b=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d=11, x=37: 00000100101 : 0011000101 : 0001100   \n"
     ]
    }
   ],
   "source": [
    "from math import log,ceil\n",
    "\n",
    "log2 = lambda x: log(x,2)\n",
    "\n",
    "def binary(x,l=1):\n",
    "    fmt = '{0:0%db}' % l\n",
    "    return fmt.format(x)\n",
    "\n",
    "def binarytodecimal(x):\n",
    "    return int(x,2)  \n",
    "\n",
    "def unary(x):\n",
    "    return (x)*'0'\n",
    "\n",
    "def elias_gamma_encoding(x):\n",
    "    temp = binary(x)\n",
    "    i = unary(len(temp)-1)\n",
    "    return i + temp\n",
    "\n",
    "def elias_delta_encoding(x):\n",
    "    if x == 0: return '0'\n",
    "    l = 1+int(log2(x))\n",
    "    temp1 = elias_gamma_encoding(l)\n",
    "    temp2 = binary(x)\n",
    "    return temp1+temp2[1:]\n",
    "\n",
    "def is_power(n):\n",
    "    n = n/2\n",
    "    if n == 2:\n",
    "        return True\n",
    "    elif n > 2:\n",
    "        return is_power(n)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def golomb_encoding(b, x):\n",
    "    q = int((x) / b)\n",
    "    i = int((log2(b)))\n",
    "    d = 2**(i+1)-b\n",
    "    r = x-q*b\n",
    "    if is_power(b) or r<d:\n",
    "        return unary(q)+'1'+binary(r, i)\n",
    "    else:\n",
    "        return unary(q)+ '1' + binary(r+d, i+1)\n",
    "\n",
    "i = 37\n",
    "print(\"b=11, x=37: %-10s : %-10s : %-10s\" %(elias_gamma_encoding(i),elias_delta_encoding(i), golomb_encoding(11,i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d=8, x=37: 00000100101 : 0011000101 : 00001101  \n"
     ]
    }
   ],
   "source": [
    "print(\"b=8, x=37: %-10s : %-10s : %-10s\" %(elias_gamma_encoding(i),elias_delta_encoding(i), golomb_encoding(8,i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elias_gamma_decoding(x):\n",
    "    return binarytodecimal(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elias_gamma_decoding('00000100101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elias_delta_decoding(x):\n",
    "    temp = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != '1':\n",
    "            temp += 1\n",
    "        else:\n",
    "            break\n",
    "    num = x[2*temp+1:]\n",
    "    return binarytodecimal('1'+num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elias_delta_decoding('0011000101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golomb_decoding(b, x):\n",
    "    q = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != '1':\n",
    "            q += 1\n",
    "        else:\n",
    "            break\n",
    "    i = int((log2(b)))\n",
    "    d = 2**(i+1)-b\n",
    "    r = x[q+1:q+i+1]\n",
    "    if binarytodecimal(r) >= d:\n",
    "        r = r+x[q+i+1]\n",
    "        r = binarytodecimal(r) - d\n",
    "    else:\n",
    "        r = binarytodecimal(r)\n",
    "    return q*b+r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golomb_decoding(11, '0001100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4: Based on the contents of those two documents retrieved in Q. No. 3, prepare one inverted index (with proper representation) and save it in another text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = open(\"web_mining.doc\", \"r\", encoding='utf-8')\n",
    "doc2 = open(\"data_mining.doc\", \"r\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups1 = {}\n",
    "groups2 = {} \n",
    "ind = 0\n",
    "line1 = doc1.read()\n",
    "line2 = doc2.read()\n",
    "line1 = line1.split()\n",
    "line2 = line2.split()\n",
    "\n",
    "for word in alltext:\n",
    "    if word in words.words(): \n",
    "        for word2 in line1:   \n",
    "            if(word == word2):\n",
    "                if word not in groups1.keys():\n",
    "                    groups1[word] = list()\n",
    "                groups1[word].append((ind))\n",
    "            ind+=len(word2)+1\n",
    "        if word not in groups1.keys():\n",
    "            groups1[word] = list()\n",
    "    \n",
    "    doc1.seek(0, 0)\n",
    "    \n",
    "    ind = 0\n",
    "    \n",
    "    if word in words.words(): \n",
    "        for word2 in line2:   \n",
    "            if(word == word2):\n",
    "                if word not in groups1.keys():\n",
    "                    groups1[word] = list()\n",
    "                groups1[word].append((ind))\n",
    "            ind+=len(word2)+1\n",
    "        if word not in groups2.keys():\n",
    "            groups2[word] = list()\n",
    "    \n",
    "    doc2.seek(0, 0)\n",
    "    \n",
    "    ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_text = list()\n",
    "for word in alltext:\n",
    "    if word in words.words():\n",
    "        filt_text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1 = list()\n",
    "for key in list(groups1.keys()):\n",
    "    temp = [\"Doc1\", len(groups1[key]), groups1[key]]\n",
    "    pos1.append(temp)\n",
    "\n",
    "pos2 = list()\n",
    "for key in list(groups2.keys()):\n",
    "    temp = [\"Doc2\", len(groups2[key]), groups2[key]]\n",
    "    pos2.append(temp)\n",
    "\n",
    "pos = [None]*len(pos1)\n",
    "for i in range(len(pos1)):\n",
    "    pos[i] = [pos1[i], pos2[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = pd.DataFrame({'Word': filt_text, 'Inverted Index': pos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Inverted Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>variety</td>\n",
       "      <td>[[Doc1, 1, [5877]], [Doc2, 0, []]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>[[Doc1, 1, [37306]], [Doc2, 0, []]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capability</td>\n",
       "      <td>[[Doc1, 2, [4870, 5943]], [Doc2, 0, []]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resulting</td>\n",
       "      <td>[[Doc1, 1, [13225]], [Doc2, 0, []]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>[[Doc1, 2, [11624, 12837]], [Doc2, 0, []]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word                              Inverted Index\n",
       "0       variety          [[Doc1, 1, [5877]], [Doc2, 0, []]]\n",
       "1  acceleration         [[Doc1, 1, [37306]], [Doc2, 0, []]]\n",
       "2    capability    [[Doc1, 2, [4870, 5943]], [Doc2, 0, []]]\n",
       "3     resulting         [[Doc1, 1, [13225]], [Doc2, 0, []]]\n",
       "4           all  [[Doc1, 2, [11624, 12837]], [Doc2, 0, []]]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index.to_csv(\"inverted_index.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1: Write a program to show the implementation of a topical cralwer. Develop the crawler program to handle various challenges (such as Parsing, Stemming, Lemmitization, Link Extraction, Canonicalization, Spider Trap etc.) faced by crawler while implementing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = pd.DataFrame(columns=[\"Company Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "domain = \"https://internshala.com\"\n",
    "shala = \"https://internshala.com/internships/computer%20science-internship-in-delhi,greater%20noida,gurgaon,gurugram,new%20delhi,noida,south%20west%20delhi,north%20delhi\"\n",
    "i=0\n",
    "while(shala!=None):\n",
    "    response = get(shala, headers = headers)\n",
    "    print(response)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    company_containers = html_soup.find_all('a', class_ = \"link_display_like_text\")\n",
    "    first = company_containers[0]\n",
    "    for comp in company_containers:\n",
    "        company.loc[i] = comp.contents[0].strip()\n",
    "        i+=1\n",
    "    next_page_sec = html_soup.find_all('ul', class_ = \"pager\")\n",
    "    next_page = (next_page_sec[0].find_all('a')[1]['href'])\n",
    "    if next_page == \"#\":\n",
    "        shala = None\n",
    "    else:\n",
    "        shala = domain+next_page\n",
    "    #company = temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoCubes Technologies Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aspiring Minds Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viacom India LLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CL Educate Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>House Of Desi Products Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Encrobytes Technologies (OPC) Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GIFOLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F2C India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Twango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BuildnLive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thinkbotic Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Devkraft Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Logiclump Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ravtek India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Allied Infoline Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Randomly Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vplak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gadgets Geek India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Liamliang Travellers Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AJIVIKA FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Totality Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SilverMile Capital Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Global Groupware Solutions Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Quantum IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Craxy Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Script Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Neolen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Perito Staffing Services LLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Create Bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Imparity Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Hipstercart Shopping Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>OpenCubicles Technologies Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Devtron Labs Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Famous Dogg Studios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>The Quick Bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Blackmelon Advance Technology Co. Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Interactive Pixels Digital Solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Credicxo Tech Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Appexigo Technologies Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>OpenSense Labs Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Trateners Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Rising Star Khilte Chehre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Branding Pioneers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Nisin Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Project Assignment Solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Wehyphens Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Pentashiva Infraventures Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Carpathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>BoticX Labs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Moolcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>TBM Technologies Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>Internshala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>MNJ Software Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Zuneeue Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Couch Fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>SPAAK Super Tec Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>W8intern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Naxap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Observer Research Foundation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>The Indian Institute For Human Settlements (IIHS)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Company Name\n",
       "0                 CoCubes Technologies Private Limited\n",
       "1                       Aspiring Minds Private Limited\n",
       "2                                     Viacom India LLP\n",
       "3                                   CL Educate Limited\n",
       "5               House Of Desi Products Private Limited\n",
       "6        Encrobytes Technologies (OPC) Private Limited\n",
       "7                                               GIFOLA\n",
       "8                                            F2C India\n",
       "9                                               Twango\n",
       "11                                          BuildnLive\n",
       "12                               Thinkbotic Technology\n",
       "13                               Devkraft Technologies\n",
       "14                              Logiclump Technologies\n",
       "15                                        Ravtek India\n",
       "16                     Allied Infoline Private Limited\n",
       "17                                       Randomly Club\n",
       "18                                               Vplak\n",
       "19                                  Gadgets Geek India\n",
       "20                Liamliang Travellers Private Limited\n",
       "22                                     AJIVIKA FINANCE\n",
       "23                                Totality Corporation\n",
       "24                  SilverMile Capital Private Limited\n",
       "25                  Global Groupware Solutions Limited\n",
       "26                                          Quantum IT\n",
       "27                                         Craxy Store\n",
       "29                                   Script Technology\n",
       "30                                              Neolen\n",
       "31                        Perito Staffing Services LLP\n",
       "32                                        Create Bytes\n",
       "34                                   Imparity Software\n",
       "..                                                 ...\n",
       "431               Hipstercart Shopping Private Limited\n",
       "432          OpenCubicles Technologies Private Limited\n",
       "433                       Devtron Labs Private Limited\n",
       "436                                Famous Dogg Studios\n",
       "437                                     The Quick Bowl\n",
       "438  Blackmelon Advance Technology Co. Private Limited\n",
       "440               Interactive Pixels Digital Solutions\n",
       "441                      Credicxo Tech Private Limited\n",
       "442              Appexigo Technologies Private Limited\n",
       "443                     OpenSense Labs Private Limited\n",
       "444                          Trateners Private Limited\n",
       "445                          Rising Star Khilte Chehre\n",
       "447                                  Branding Pioneers\n",
       "448                                 Nisin Technologies\n",
       "450                       Project Assignment Solutions\n",
       "451                          Wehyphens Private Limited\n",
       "453           Pentashiva Infraventures Private Limited\n",
       "454                                           Carpathy\n",
       "455                                        BoticX Labs\n",
       "456                                           Moolcode\n",
       "457                   TBM Technologies Private Limited\n",
       "461                                        Internshala\n",
       "462                       MNJ Software Private Limited\n",
       "464                               Zuneeue Technologies\n",
       "466                                      Couch Fashion\n",
       "472                    SPAAK Super Tec Private Limited\n",
       "474                                           W8intern\n",
       "479                                              Naxap\n",
       "480                       Observer Research Foundation\n",
       "481  The Indian Institute For Human Settlements (IIHS)\n",
       "\n",
       "[356 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3: Write a program to extract the contents (excluding any tags) from two websites (https://en.wikipedia.org/wiki/Web_mining&https://en.wikipedia.org/wiki/Data_mining) and save the content in two separate .doc file. Construct a trie based on the content retrieved in Q. No. 3 using HashMap. Write a program to show the implementation of Predictive Typing and Auto-Correct using the trie prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url1 = requests.get(\"https://en.wikipedia.org/wiki/Web_mining\")\n",
    "url2 = requests.get(\"https://en.wikipedia.org/wiki/Data_mining\")\n",
    "doc1 = open(\"web_mining.doc\", \"a\", encoding='utf-8')\n",
    "doc2 = open(\"data_mining.doc\", \"a\", encoding='utf-8')\n",
    "soup1 = BeautifulSoup(url1.text)\n",
    "soup2 = BeautifulSoup(url2.text)\n",
    "for script in soup1([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "for script in soup2([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "text1 = soup1.get_text()\n",
    "text2 = soup2.get_text()\n",
    "\n",
    "lines1 = (line.strip() for line in text1.splitlines())\n",
    "chunks1 = (phrase.strip() for line in lines1 for phrase in line.split(\"  \"))\n",
    "text1 = '\\n'.join(chunk for chunk in chunks1 if chunk)\n",
    "text1 = text1.lower()\n",
    "text1 = text1.split(\"\\n\")\n",
    "for line in text1:\n",
    "    temp = line.split()\n",
    "    for i in temp:\n",
    "        doc1.write(i+\" \")\n",
    "\n",
    "lines2 = (line.strip() for line in text2.splitlines())\n",
    "chunks2 = (phrase.strip() for line in lines2 for phrase in line.split(\"  \"))\n",
    "text2 = '\\n'.join(chunk for chunk in chunks2 if chunk)\n",
    "text2 = text2.lower()\n",
    "text2 = text2.split(\"\\n\")\n",
    "for line in text2:\n",
    "    temp = line.split()\n",
    "    for i in temp:\n",
    "        doc2.write(i+\" \")\n",
    "        \n",
    "text = list()\n",
    "for line in text1:\n",
    "    temp = line.split()\n",
    "    for i in temp:\n",
    "        text.append(i)\n",
    "for line in text2:\n",
    "    temp = line.split()\n",
    "    for i in temp:\n",
    "        text.append(i)\n",
    "temp = set(text)\n",
    "alltext = list(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Achyut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.close()\n",
    "doc2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Phrase: This is\n",
      "\n",
      "\n",
      "Suggestion for Predictive Typing :  ['is', 'issue']\n",
      "\n",
      "\n",
      "Spelled Incorrectly, it Should be  []\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, label=None, data=None):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.children = dict()\n",
    "    \n",
    "    def addChild(self, key, data=None):\n",
    "        if not isinstance(key, Node):\n",
    "            self.children[key] = Node(key, data)\n",
    "        else:\n",
    "            self.children[key.label] = key\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.children[key]\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.head = Node()\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.head.children[key]\n",
    "    \n",
    "    def add(self, word):\n",
    "        current_node = self.head\n",
    "        word_finished = True\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            if word[i] in current_node.children:\n",
    "                current_node = current_node.children[word[i]]\n",
    "            else:\n",
    "                word_finished = False\n",
    "                break\n",
    "        if not word_finished:\n",
    "            while i < len(word):\n",
    "                current_node.addChild(word[i])\n",
    "                current_node = current_node.children[word[i]]\n",
    "                i += 1\n",
    "        current_node.data = word\n",
    "    \n",
    "    def has_word(self, word):\n",
    "        if word == '':\n",
    "            return False\n",
    "        if word == None:\n",
    "            raise ValueError('Trie.has_word requires a not-Null string')\n",
    "        current_node = self.head\n",
    "        exists = True\n",
    "        for letter in word:\n",
    "            if letter in current_node.children:\n",
    "                current_node = current_node.children[letter]\n",
    "            else:\n",
    "                exists = False\n",
    "                break\n",
    "        if exists:\n",
    "            if current_node.data == None:\n",
    "                exists = False\n",
    "        \n",
    "        return exists\n",
    "    \n",
    "    def start_with_prefix(self, prefix):\n",
    "        words = list()\n",
    "        if prefix == None:\n",
    "            raise ValueError('Requires not-Null prefix')\n",
    "        top_node = self.head\n",
    "        for letter in prefix:\n",
    "            if letter in top_node.children:\n",
    "                top_node = top_node.children[letter]\n",
    "            else:\n",
    "                return words\n",
    "        if top_node == self.head:\n",
    "            queue = [node for key, node in top_node.children.items()]\n",
    "        else:\n",
    "            queue = [top_node]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop()\n",
    "            if current_node.data != None:\n",
    "                words.append(current_node.data)\n",
    "            \n",
    "            queue = [node for key,node in current_node.children.items()] + queue\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def getData(self, word):\n",
    "        \"\"\" This returns the 'data' of the node identified by the given word \"\"\"\n",
    "        if not self.has_word(word):\n",
    "            raise ValueError('{} not found in trie'.format(word))\n",
    "\n",
    "        current_node = self.head\n",
    "        for letter in word:\n",
    "            current_node = current_node[letter]\n",
    "        \n",
    "        return current_node.data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trie = Trie()\n",
    "    for word in alltext:\n",
    "        if word in words.words():\n",
    "            trie.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Phrase: My nam\n",
      "\n",
      "\n",
      "Suggestion for Predictive Typing :  ['name']\n",
      "\n",
      "\n",
      "Spelled Incorrectly, it Should be  []\n",
      "Spelled Incorrectly, it Should be  ['no', 'nor', 'not', 'net', 'new', 'name', 'node', 'need', 'naive', 'noise', 'needs', 'newly', 'number', 'nature', 'normal', 'neural', 'natural', 'notable', 'neither', 'needing', 'network', 'national', 'numerical', 'necessary', 'necessity', 'navigation', 'newsletter', 'necessarily']\n"
     ]
    }
   ],
   "source": [
    "x = input(\"Enter Your Phrase: \")\n",
    "li = x.split()\n",
    "print(\"\\n\")\n",
    "print(\"Suggestion for Predictive Typing : \",trie.start_with_prefix(li[-1]))\n",
    "print(\"\\n\")\n",
    "   \n",
    "for i in range(len(li)):\n",
    "    if li[i] not in trie.start_with_prefix(li[i]):\n",
    "        print(\"Spelled Incorrectly, it Should be \", trie.start_with_prefix(li[i][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
